![MIT](https://badges.frapsoft.com/os/mit/mit.svg?v=102)
# Video-Captioning

<img align='center' style="border-color:gray;border-width:2px;border-style:dashed"   src='https://vsubhashini.github.io/imgs/S2VTarchitecture.png' padding='5px' height="250px"></img>
<a href='https://vsubhashini.github.io/imgs/S2VTarchitecture.png'>Image src</a>

An implement of the paper [Sequence to Sequence -- Video to Text](https://arxiv.org/abs/1505.00487) in Tensorflow 1.0

And add attention mechanism to it

The details was described in [__introduction.pdf__](https://github.com/brianhuang1019/Video-Captioning/blob/master/introduction.pdf).

## run the code
```bash
./run.sh <testing_video ids> <testing_video features>
```

## train the code
```bash
./train.sh
```

## test the code
```bash
./test.sh
```

## Author
Po-Chih Huang / [@brianhuang1019](http://brianhuang1019.github.io/)
